---
title:QEUR23_LLMDSS8: 簡単にFinetune を使ってみる（Ludwig）
date: 2023-09-07
tags: ["QEUシステム", "メトリックス", "Python言語", "LLM", "データセット", "Fine-tuning", "イノベーション"]
excerpt: LLMのファインチューニングを最適化する
---

## QEUR23_LLMDSS8: 簡単にFinetune を使ってみる（Ludwig）

## ～ 時代は群雄割拠！ ～

QEU:FOUNDER ： “前回の例の**「Finetuning」のソルーション**の採用は、我々としてはやめておきましょう。”

![imageJRL5-9-1](/2023-09-07-QEUR23_LLMDSS8/imageJRL5-9-1.jpg)

D先生 ： “どうして急に(採用を)やめたの？”

[![MOVIE1](http://img.youtube.com/vi/g68qlo9Izf0/0.jpg)](http://www.youtube.com/watch?v=g68qlo9Izf0 "Efficient Fine-Tuning for Llama-v2-7b on a Single GPU")

QEU:FOUNDER ： “あの御大のチャンネルが、なぜか裏番組で違うソルーションを出していましたから・・・。この場合、**量子化「Quantization」の技術を取り入れている**ので、ハードウェアに対する要求が緩いんです。なんと、**T４のGPU**でもいけるんですよ。”

```python
"""
By the end of this session, attendees will understand:
- How to fine-tune LLMs like Llama-2-7b on a single GPU
- Techniques like parameter efficient tuning and quantization, and how they can help
- How to train a 7b param model on a single T4 GPU (QLoRA)
- How to deploy tuned models like Llama-2 to production
- Continued training with RLHF
- How to use RAG to do question answering with trained LLMs
This session will equip ML engineers to unlock the capabilities of LLMs like Llama-2 on for their own projects.
このセッションを終えるまでに、参加者は次のことを理解するだろう：
- シングルGPU上でLlama-2-7bのようなLLMを微調整する方法
- パラメータ効率チューニングや量子化などのテクニックと、それらがどのように役立つか
- シングルT4 GPUで7b paramモデルをトレーニングする方法（QLoRA）
- Llama-2のようなチューニングされたモデルを実機に導入する方法
- RLHFによるトレーニングの継続
- 訓練されたLLMで質問応答を行うためにRAGを使用する方法
このセッションは、Llama-2のようなLLMの能力を、MLエンジニアが自身のプロジェクトで発揮できるようにするものです。
"""
```

D先生 ： “普通のFinetuningのやり方を使えば、**A100レベルのGPU**が必要になります。もし、T4でも行けるんでしたら、そりゃ、それはありがたいんじゃないですか？”

![imageJRL5-9-2](/2023-09-07-QEUR23_LLMDSS8/imageJRL5-9-2.jpg)

QEU:FOUNDER ： “このYoutubeのコメントに紹介されているプログラムを動かせば、何とかなりますよ。ちなみに、**このプログラムはwgetでGithubから関数パッケージを持ってこないと動きません**。つまり、このプログラムは動きようのない、事実上は意味のない呪文群ですよ。それでは、プログラムをドン！！”

```python
# -----
#import getpass
import locale; locale.getpreferredencoding = lambda: "UTF-8"
import logging
import torch
import yaml
# -----
from ludwig.api import LudwigModel

# ------
# コード生成データセットのインポート
from google.colab import data_table; data_table.enable_dataframe_formatter()
import numpy as np; np.random.seed(123)
import pandas as pd

df = pd.read_json("https://raw.githubusercontent.com/sahil280114/codealpaca/master/data/code_alpaca_20k.json")

# We're going to create a new column called `split` where:
# 90% will be assigned a value of 0 -> train set
# 5% will be assigned a value of 1 -> validation set
# 5% will be assigned a value of 2 -> test set
# Calculate the number of rows for each split value
total_rows = len(df)
split_0_count = int(total_rows * 0.9)
split_1_count = int(total_rows * 0.05)
split_2_count = total_rows - split_0_count - split_1_count

# Create an array with split values based on the counts
split_values = np.concatenate([
    np.zeros(split_0_count),
    np.ones(split_1_count),
    np.full(split_2_count, 2)
])

# Shuffle the array to ensure randomness
np.random.shuffle(split_values)

# Add the 'split' column to the DataFrame
df['split'] = split_values
df['split'] = df['split'].astype(int)

# For this webinar, we will just 500 rows of this dataset.
df = df.head(n=500)

# コード・アルパカのデータセットを理解する
df.head(10)

```

C部長 : “ん！？なにこれ？”

![imageJRL5-9-3](/2023-09-07-QEUR23_LLMDSS8/imageJRL5-9-3.jpg)

QEU:FOUNDER ： “Alpacaデータセットには、本来は数万のデータがあるんですが、そのうち**一部だけを学習に使う**ようですね。それでは、続きにいきましょう。”

```python
# -----
# QLoRA
model = None
clear_cache()

qlora_fine_tuning_config = yaml.safe_load(
"""
model_type: llm
base_model: huggyllama/llama-7b

input_features:
  - name: instruction
    type: text

output_features:
  - name: output
    type: text

prompt:
  template: >-
    Below is an instruction that describes a task, paired with an input
    that provides further context. Write a response that appropriately
    completes the request.

    ### Instruction: {instruction}

    ### Input: {input}

    ### Response:

generation:
  temperature: 0.1
  max_new_tokens: 512

adapter:
  type: lora

quantization:
  bits: 4

trainer:
  type: finetune
  epochs: 5
  batch_size: 1
  eval_batch_size: 2
  gradient_accumulation_steps: 16
  learning_rate: 0.0001
  optimizer:
    type: adam
    params:
      eps: 1.e-8
      betas:
        - 0.9
        - 0.999
      weight_decay: 0
  learning_rate_scheduler:
    warmup_fraction: 0.03
    reduce_on_plateau: 0
"""
)

model = LudwigModel(config=qlora_fine_tuning_config, logging_level=logging.INFO)
results = model.train(dataset=df)

```

C部長 : “学習が始まりましたね。これは簡単だ。”

![imageJRL5-9-4](/2023-09-07-QEUR23_LLMDSS8/imageJRL5-9-4.jpg)

QEU:FOUNDER  ： “確かに動きが軽いですね。T4で十分動きますし、**エポック５回で３０分**かかりました。”

![imageJRL5-9-5](/2023-09-07-QEUR23_LLMDSS8/imageJRL5-9-5.jpg)

D先生 ： “逆に言うと、**計算時間から逆算すると１万件ぐらいの学習データ量が実用上で最大**になりますかね？”

QEU:FOUNDER  ： “我々が貧乏だから、qLoRAを使っているわけで・・・。お金を使えば、別の方法があるんじゃないかなぁ・・・。あとは、学習したモデルで推論をしてみましょう。”

```python
# ----
test_examples = pd.DataFrame([
      {
            "instruction": "Create an array of length 5 which contains all even numbers between 1 and 10.",
            "input": ''
      },
      {
            "instruction": "Create an array of length 15 containing numbers divisible by 3 up to 45.",
            "input": "",
      },
      {
            "instruction": "Create a nested loop to print every combination of numbers between 0-9",
            "input": ""
      },
      {
            "instruction": "Generate a function that computes the sum of the numbers in a given list",
            "input": "",
      },
      {
            "instruction": "Create a class to store student names, ages and grades.",
            "input": "",
      },
      {
            "instruction": "Print out the values in the following dictionary.",
            "input": "my_dict = {\n  'name': 'John Doe',\n  'age': 32,\n  'city': 'New York'\n}",
      },
])

predictions = model.predict(test_examples)[0]
for input_with_prediction in zip(test_examples['instruction'], test_examples['input'], predic-tions['output_response']):
  print(f"Instruction: {input_with_prediction[0]}")
  print(f"Input: {input_with_prediction[1]}")
  print(f"Generated Output: {input_with_prediction[2][0]}")
  print("\n\n")

```

D先生 ： “一応は推論の結果が出ていますね。”

![imageJRL5-9-6](/2023-09-07-QEUR23_LLMDSS8/imageJRL5-9-6.jpg)

QEU:FOUNDER  ： “今回はAlpacaデータで学習をしましたが、次は**自分のデータ**を使います。やりたいことがあるんでね。ちなみに、このソルーションも自分のプラットフォームを売っているらしいです。もし、**それを買えばもうちょっと「ご利益」があるかも**しれない・・・。”

[＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038
)

C部長 : “誰かから寄付があれば、ぜひこのサービスを使ってみたいですね。”


## ～ まとめ ～

### ・・・ 前回のつづきです（「安いが一番の巻」） ・・・

D先生 ： “例によって、この件（↓）で世の中が盛り上がっていますが・・・。”

![imageJRL5-9-7](/2023-09-07-QEUR23_LLMDSS8/imageJRL5-9-7.jpg)

QEU:FOUNDER ： “あの**「おじさん」**・・・。さすがに、A国の大学院で勉強しただけあって、英語がお上手ですね。”

[![MOVIE2](http://img.youtube.com/vi/zPYyWS16lbc/0.jpg)](http://www.youtube.com/watch?v=zPYyWS16lbc "12 Misdecisions and Misinformation of the Treated Radioactive Water Ocean Discharge from Fukushima")

D先生 ： “なんでアレを**「出しちゃった」**んでしょうか？”

![imageJRL5-9-8](/2023-09-07-QEUR23_LLMDSS8/imageJRL5-9-8.jpg)

QEU:FOUNDER ： “**安いから**らしい。これが、J国の技術力！”

![imageJRL5-9-9](/2023-09-07-QEUR23_LLMDSS8/imageJRL5-9-9.jpg)

D先生 ： “**世界第２位の技術**ね（笑）。”

[![MOVIE3](http://img.youtube.com/vi/Krl9jQqGzpw/0.jpg)](http://www.youtube.com/watch?v=Krl9jQqGzpw "テスラは巨大な発電会社／株価はバブルではない／企業価値が高い3つの理由／ソフト開発力はトヨタの100倍")

QEU:FOUNDER ： “安いから。これが**J国の技術力**！”



