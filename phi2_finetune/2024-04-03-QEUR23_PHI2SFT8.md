---
title:QEUR23_PHI2SFT8: 閑話休題～LLM学習(Finetuning)で検証データを追加する
date: 2024-04-03
tags: ["QEUシステム", "メトリックス", "Python言語", "LLM", "データセット", "Fine-tuning", "イノベーション","PHI-2"]
excerpt: LLMのファインチューニングを最適化する
---

## QEUR23_PHI2SFT8: 閑話休題～LLM学習(Finetuning)で検証データを追加する

## ～ いやぁ、しかし・・・。今回はゆるいねえ・・・。 ～

QEU:FOUNDER（設定年齢65歳）  ： “じゃあ、つぎは検証データ(_eval)のデータセットを準備したのでつかってみましょう。”

**（Evaluationデータ）**

![imageJRP2-9-1](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-1.jpg)

D先生（設定年齢65歳） ： “ちなみに、いままでは学習用データセット(_train)のみでプログラムを動かしていました。でも、考えてみれば、いままでのやり方がおかしかったわけで・・・。”

**（Trainデータ）**

![imageJRP2-9-2](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-2.jpg)

QEU:FOUNDER（設定年齢65歳）  ： “それでも、インターネットで調べてみると、ほとんど皆が普通にtrainのみでSFT学習していますよ。ちなみに、今回の場合には、以下のプログラムの一部分のみがかわります。”

```python
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset_train,
    eval_dataset=dataset_eval,
    peft_config=peft_config,
    max_seq_length= 2048,
    tokenizer=tokenizer,
    args=training_arguments,
    formatting_func=formatting_prompts_func,
    packing= False,
)

trainer.train()
```

QEU:FOUNDER ： “今回もMITモデルです。我々の今回のプロジェクトは「ぜ～んぶ」ね・・・。”

![imageJRP2-9-3](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-3.jpg)

### （重要：MITライセンスへのリンク：これからは、ず～っと使うよ）

###[(クリックしてね！)](https://opensource.org/license/MIT)

D先生 ： “evalデータセットを使うと何が良くなるのか、HFの解説をいてもようわからんのよ・・・。・・・で、この学習結果はどうなりました？”

![imageJRP2-9-4](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-4.jpg)

QEU:FOUNDER ： “感覚的には、前回とほとんど変わっていません。おそらく、**学習したデータが少しだけ増えただけ**のメリットでしょうね。”

![imageJRP2-9-5](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-5.jpg)

D先生（設定年齢65歳） ： “まあ、あまり期待せずに結果をみてみましょう。”

QEU:FOUNDER  ： “そういってもらえるとありがたい。まずは、我々のいうところの「推論方式A」をやってみましょう。”

```python
# ---
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=300)
result = pipe(prompt)
print(result[0]['generated_text'])
```

**(推論A)**

![imageJRP2-9-6](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-6.jpg)

D先生 ： “全然だめだわ・・・。じゃあ、「推論方式B」はこうなったと・・・。”

```python
# ---
# プロンプトを生成する(推論タイプB)
def generate_text(query):
    inputs = tokenizer(query, 
                       return_tensors="pt", 
                       return_attention_mask=False).to('cuda')
    outputs = model.generate(**inputs, max_length=200)
    text = tokenizer.batch_decode(outputs)[0]
    return text

generated_text = generate_text(prompt)
print(generated_text)

```

**（推論B）**

![imageJRP2-9-7](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-7.jpg)

QEU:FOUNDER ： “ひとこと。「前回とほとんど変わらん」とも言えますね。じゃあ、他の質問をしてみましょう。ここでは、最終目標は「A国の首都を質問すること」がポイントです。そのときに、ＬＬＭはE語のモデル脳を使っているはずなので・・・。その前にJ国の首都を質問した場合を見てみるとこうなります（↓）。”

**（推論のAタイプ）**

![imageJRP2-9-8](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-8.jpg)

**（推論のBタイプ）**

![imageJRP2-9-9](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-9.jpg)

D先生 ： “**初めて「まともな答え」がでた**・・・。たまたまとはいえ、これは感動です。”

QEU:FOUNDER（設定年齢65歳）  ： “じゃあ、A国の首都の場合は？”

**（推論のAタイプ）**

![imageJRP2-9-10](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-10.jpg)

**（推論のBタイプ）**

![imageJRP2-9-11](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-11.jpg)

D先生 ： “あれ？またもや、C国語が出ている？前回で分かっていたとはいえ、**Phi2のLLMモデルって言語（J-C）をより分ける能力がない**んですね。”

QEU:FOUNDER  ： “そりゃそうでしょ？ＬＬＭモデルって、「機械学習で生成したトークン」だけをみているんだから・・・。次に、今までやっていなかった翻訳をやってみましょう。EからJです。翻訳って、あきらかに言語を意識しますからね。”

**（推論のAタイプ）**

![imageJRP2-9-12](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-12.jpg)

**（推論のBタイプ）**

![imageJRP2-9-13](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-13.jpg)

D先生 ： “このLLMは、J語を出力するのは苦手ですね。しかも、Reasoningが間違っています。”

QEU:FOUNDER  ： “そう・・・、そのReasoningがポイントです。Reasoningを見ると、LLMモデルが何を考えているのかがわかるんです。つづいて、JからEの場合をやってみましょう。”

**（推論のAタイプ）**

![imageJRP2-9-14](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-14.jpg)

**（推論のBタイプ）**

![imageJRP2-9-15](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-15.jpg)

D先生 ： “Reasoningを見てみるとＬＬＭが誤解をしています。ただし、正しい答えを出している例もあります。”

QEU:FOUNDER ： “Ｄ先生・・・。**Reasoning（推論プロンプト）って、大事でしょう？**”

D先生 ： “Reasoningをうまく使うと、出力が安定化する期待が持てます。”

QEU:FOUNDER ： “J語とC語を分離する必要もあるんです。道は長いなあ・・・。やりたいことがありすぎて、体がついていっていません。今回は、ゆる～いプロジェクトです。次に進めましょう。”

### [＞寄付のお願い(click here)＜](https://www.paypal.com/paypalme/QEUglobal?v=1&utm_source=unp&utm_medium=email&utm_campaign=RT000481&utm_unptid=29844400-7613-11ec-ac72-3cfdfef0498d&ppid=RT000481&cnac=HK&rsta=en_GB%28en-HK%29&cust=5QPFDMW9B2T7Q&unptid=29844400-7613-11ec-ac72-3cfdfef0498d&calc=f860991d89600&unp_tpcid=ppme-social-business-profile-creat-ed&page=main%3Aemail%3ART000481&pgrp=main%3Aemail&e=cl&mchn=em&s=ci&mail=sys&appVersion=1.71.0&xt=104038)

QEU:FOUNDER ： “次はさらにデータセットを改善しましょう。道は遠いね・・・。”

## ～ まとめ ～

QEU:FOUNDER ： “とうとう、Ｃ国が本気を出してきました。これ（↓）か・・・。”

[![MOVIE1](http://img.youtube.com/vi/S3ip5NkK1Lo/0.jpg)](http://www.youtube.com/watch?v=S3ip5NkK1Lo "15分朝刊チェック！：能登地震の復興を妨げるもの")

C部長 : “Ａ国のＴ社もガクブルでしょう・・・。突然Ｔ社の値下げとか、Ａ社の撤退の意味が見えてきますね。”

![imageJRP2-9-16](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-16.jpg)

QEU:FOUNDER ： “ついでに言うと、こんなもの（↓）も見えました。Ｃ部長って、**あの世代なの**？”

![imageJRP2-9-17](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-17.jpg)

C部長**（設定は50台前半です）** : “そうですが、何か・・・？”

QEU:FOUNDER ： “今、君の会社の外ではこうなっているそうですよ。”

![imageJRP2-9-18](/2024-04-03-QEUR23_PHI2SFT8/imageJRP2-9-18.jpg)

C部長 : “**（ガクガク・・・、ブルブル・・・。）**”

QEU:FOUNDER ： “今日は、これぐらいにしましょう。Ｃ先生、気をつけて帰ってね・・・（笑）。”

